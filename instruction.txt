# Streamlit ML Automation App Instructions

## Overview Instruction

Develop a **Streamlit** application that automates the entire machine learning (ML) pipeline, enabling users to understand the ML building process and obtain prediction results seamlessly. The app will allow users to:

- Upload datasets
- Process data
- Engineer features
- Build and compare ML models
- Save models for reproducibility
- Generate predictions

## Core Functionalities

### 1. Data Uploading

Enable users to upload their datasets or use predefined example data for building ML models.

#### 1.1 Upload CSV

1. Allow users to upload datasets in **CSV** format.
2. Enforce a maximum file size limit of **10MB**.
3. Validate the uploaded file to ensure it is in CSV format.
4. Read the uploaded data into a **Pandas DataFrame**.

#### 1.2 Use Example Data

1. Provide an option to use example data.
2. Load the example dataset from the following URL: https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv

3. Read the example data into a **Pandas DataFrame**.

#### 1.3 User Interface

1. Present a clear choice between uploading a CSV file and using example data.
2. Display a preview of the uploaded or example dataset (e.g., first 5 rows).

### 2. Data Processing

Ensure the dataset’s quality and prepare it for feature engineering and model building.

#### 2.1 Initiate Data Processing

1. After data upload, prompt the user to confirm whether to proceed with data processing.
2. Provide an option (e.g., a “Start Data Processing” button) to initiate the next steps.

#### 2.2 Target Variable and Features Selection

1. Allow users to specify the **target variable** (dependent variable) and **features** (independent variables).
2. Validate the selected target variable to ensure it is appropriate for the ML task (e.g., time series prediction, classification, or regression).
3. Check for **class imbalance** in the target variable (e.g., using class distribution plots).
- If imbalance is detected, notify the user and suggest possible remedies (e.g., resampling techniques).

#### 2.3 Missing Values Analysis

1. Identify missing values in the dataset.
2. Visualize missing data patterns (e.g., using heatmaps or bar charts).
3. Provide options for handling missing values, such as:
- **Imputation Methods:**
  1. Mean, median, mode for numerical features.
  2. Most frequent or constant value for categorical features.
  3. For time series prediction, consider forward or backward filling.
- **Drop Missing Values:**
  - Allow users to drop rows or columns with missing values if appropriate (e.g., if certain columns have over 50% missing values, making imputation less meaningful).

#### 2.4 Data Quality Checks

1. Detect and handle outliers in numerical features (e.g., using box plots).
2. Ensure data types are correctly inferred and converted as needed.

### 3. Feature Engineering

Transform and create features to enhance model performance.

#### 3.1 Categorical Feature Encoding

1. Identify categorical variables.
2. Apply **One-Hot Encoding** or **Label Encoding** as appropriate.

#### 3.2 Numerical Feature Scaling

1. Implement scaling techniques such as **StandardScaler** or **MinMaxScaler**.

#### 3.3 Feature Selection

1. Provide methods for selecting important features (e.g., correlation analysis, feature importance scores).
2. Allow users to exclude irrelevant or redundant features.

### 4. Model Building

Build, train, and select the best-performing ML models based on user preferences and data characteristics.

#### 4.1 Model Selection

1. Provide a list of supported ML models, including but not limited to:
- **Regression Models:** Linear Regression, Ridge, Lasso
- **Classification Models:** Logistic Regression, Decision Trees, Random Forest, XGBoost

#### 4.2 Model Validation

1. Determine the type of ML problem (classification or regression) based on the target variable.
2. Validate that selected models are appropriate for the problem type (e.g., prevent selecting Linear Regression for classification).

#### 4.3 Hyperparameter Tuning

1. Implement hyperparameter tuning for each selected model using techniques like **Grid Search** or **Random Search**.

#### 4.4 Training Process

1. Split the dataset into training and testing sets (e.g., 80/20 split).
2. Optionally, implement **cross-validation** (e.g., k-fold cross-validation) to ensure robust model evaluation.
3. Train each model with the specified configurations.
4. Display training progress and status.

### 5. Performance and Interpretability

Provide insights into model performance and feature importance to help users understand and trust the models.

#### 5.1 Model Performance Comparison

1. Display evaluation metrics for each model, such as:
- **Classification Metrics:** Accuracy, Precision, Recall, F1-Score, ROC-AUC
- **Regression Metrics:** Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R² Score
2. Visualize performance comparisons using bar charts or other suitable plots.

#### 5.2 Feature Importance

1. Generate and display feature importance charts for each model.
2. Use methodologies like **SHAP values** or **Gini Importance** (for tree-based models).
3. Allow users to interact with the feature importance plots (e.g., hover to see exact values).

#### 5.3 Confusion Matrix and ROC Curves (for Classification)

1. Provide confusion matrices for classification models.
2. Plot ROC curves and display AUC scores.

#### 5.4 Residual Plots (for Regression)

1. Show residual plots to assess model performance and identify potential issues.

### 6. Prediction

Enable users to generate predictions using the trained models on new datasets.

#### 6.1 Model Saving

1. Serialize and save trained models using formats like **Pickle** or **Joblib**.
2. Provide an option for users to download the serialized model files for future use.

#### 6.2 Prediction Interface

1. Create a dedicated section for users to upload a **prediction dataset** (CSV format).
2. Validate that the prediction dataset matches the training dataset’s feature requirements (e.g., same feature names and types).
3. Handle discrepancies by informing users of mismatches and providing guidance.

#### 6.3 Generating Predictions

1. Apply the saved/pre-trained model to the uploaded prediction dataset.
2. Display prediction results in a tabular format within the app.
3. Allow users to download the prediction results as a CSV file.

### 7. Reproducibility

Ensure that users can reproduce their ML experiments and share their models.

#### 7.1 Model Serialization

1. Save models as serialized files (e.g., Pickle, Joblib).
2. Ensure that all necessary components (e.g., preprocessing steps, feature encoders) are included in the serialization for full reproducibility.

#### 7.2 Download Options

1. Provide download links for serialized models.
2. Offer documentation or metadata files that describe the model configurations and training parameters.

## Non-Functional Requirements

### 1. Performance

#### 1.1 Efficiency

1. Optimize data processing and model training steps to handle datasets efficiently.
2. Avoid over-complicated models that can lead to long training times.

#### 1.2 Scalability

1. Ensure the app can handle datasets up to **1 million rows** without significant performance degradation.

#### 1.3 Responsiveness

1. Maintain a responsive user interface, providing feedback during long-running operations (e.g., progress bars).

### 2. Security

#### 2.1 Data Privacy

1. Prevent the use of **Personally Identifiable Information (PII)** within the app.
2. Implement data validation to detect and restrict PII inputs.

#### 2.2 Authentication

1. If user accounts are implemented, ensure secure authentication and authorization mechanisms.

#### 2.3 Data Protection

1. Securely handle and store uploaded datasets and generated models to prevent unauthorized access.

### 3. Error Handling

#### 3.1 Robustness

1. Implement comprehensive error handling to manage unexpected issues gracefully.

#### 3.2 Logging

1. Capture and save logs of errors and exceptions to facilitate debugging.
2. Provide users with clear error messages and guidance on resolving issues.

#### 3.3 User Feedback

1. Inform users of errors in a user-friendly manner without exposing sensitive technical details.

### 4. Code Structure

1. Organize the application into **modular components** (e.g., separate modules for data processing, feature engineering, modeling) to enhance maintainability and scalability.

## Additional Recommendations

1. **Modular Code Structure:**
- Organize the application into separate modules for different functionalities to enhance maintainability and scalability.

2. **Documentation and Help:**
- Include in-app documentation or tooltips to assist users in understanding each step of the ML pipeline.
- Provide example workflows to guide users through typical use cases.

3. **Testing:**
- Implement unit tests and integration tests to ensure each functionality works as intended.
- Conduct user testing to gather feedback and improve the app's usability.

4. **Version Control:**
- Use version control systems (e.g., Git) to manage code changes and collaborate effectively.